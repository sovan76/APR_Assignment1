{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Task 1 - KNN Classifier from Scratch + Bonus Decision Boundary\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------------\n",
    "# 1. Load & Preprocess Data\n",
    "# ----------------------\n",
    "df = pd.read_csv(\"data.csv\")  # Ensure file is uploaded in Colab\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['id', 'Unnamed: 32'], inplace=True)\n",
    "\n",
    "# Encode target\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Separate features & labels\n",
    "X = df.drop(columns=['diagnosis']).values\n",
    "y = df['diagnosis'].values\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "# Train-test split\n",
    "np.random.seed(42)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "# ----------------------\n",
    "# 2. Distance Functions\n",
    "# ----------------------\n",
    "def euclidean(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def manhattan(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "def minkowski(a, b, p=3):\n",
    "    return np.sum(np.abs(a - b) ** p) ** (1 / p)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return 1 - (np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "def hamming_distance(a, b):\n",
    "    return np.mean(a != b)\n",
    "\n",
    "distance_funcs = {\n",
    "    \"Euclidean\": euclidean,\n",
    "    \"Manhattan\": manhattan,\n",
    "    \"Minkowski\": lambda a, b: minkowski(a, b, p=3),\n",
    "    \"Cosine\": cosine_similarity,\n",
    "    \"Hamming\": hamming_distance\n",
    "}\n",
    "\n",
    "# ----------------------\n",
    "# 3. KNN Implementation\n",
    "# ----------------------\n",
    "def knn_predict(X_train, y_train, x_test, k, dist_func):\n",
    "    distances = [dist_func(x_test, x_train) for x_train in X_train]\n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    k_labels = [y_train[i] for i in k_indices]\n",
    "    return Counter(k_labels).most_common(1)[0][0]\n",
    "\n",
    "def knn_evaluate(X_train, y_train, X_test, y_test, k, dist_func):\n",
    "    y_pred = [knn_predict(X_train, y_train, x, k, dist_func) for x in X_test]\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    return accuracy, np.array(y_pred)\n",
    "\n",
    "# ----------------------\n",
    "# 4. Experiment\n",
    "# ----------------------\n",
    "K_values = [3, 4, 9, 20, 47]\n",
    "results = {}\n",
    "\n",
    "for dist_name, dist_func in distance_funcs.items():\n",
    "    accuracies = []\n",
    "    for k in K_values:\n",
    "        acc, _ = knn_evaluate(X_train, y_train, X_test, y_test, k, dist_func)\n",
    "        accuracies.append(acc)\n",
    "    results[dist_name] = accuracies\n",
    "\n",
    "# ----------------------\n",
    "# 5. Best K & Distance\n",
    "# ----------------------\n",
    "best_acc = 0\n",
    "best_k = None\n",
    "best_dist = None\n",
    "best_pred = None\n",
    "\n",
    "for dist_name, dist_func in distance_funcs.items():\n",
    "    for k in K_values:\n",
    "        acc, y_pred = knn_evaluate(X_train, y_train, X_test, y_test, k, dist_func)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_k = k\n",
    "            best_dist = dist_name\n",
    "            best_pred = y_pred\n",
    "\n",
    "# ----------------------\n",
    "# 6. Confusion Matrix, Precision, Recall\n",
    "# ----------------------\n",
    "TP = np.sum((best_pred == 1) & (y_test == 1))\n",
    "TN = np.sum((best_pred == 0) & (y_test == 0))\n",
    "FP = np.sum((best_pred == 1) & (y_test == 0))\n",
    "FN = np.sum((best_pred == 0) & (y_test == 1))\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "print(\"Best K:\", best_k)\n",
    "print(\"Best Distance Metric:\", best_dist)\n",
    "print(\"Best Accuracy:\", round(best_acc * 100, 2), \"%\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP = {TP}, TN = {TN}, FP = {FP}, FN = {FN}\")\n",
    "print(\"Precision:\", round(precision, 4))\n",
    "print(\"Recall:\", round(recall, 4))\n",
    "\n",
    "# ----------------------\n",
    "# 7. Plot Accuracy vs K\n",
    "# ----------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "for dist_name, acc_list in results.items():\n",
    "    plt.plot(K_values, acc_list, marker='o', label=dist_name)\n",
    "plt.title(\"K values vs Accuracy for different Distance Metrics\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# 8. BONUS - Decision Boundary (PCA to 2D)\n",
    "# ----------------------\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2D = pca.fit_transform(X_train)\n",
    "X_test_2D = pca.transform(X_test)\n",
    "\n",
    "# Create mesh grid\n",
    "x_min, x_max = X_train_2D[:, 0].min() - 0.1, X_train_2D[:, 0].max() + 0.1\n",
    "y_min, y_max = X_train_2D[:, 1].min() - 0.1, X_train_2D[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict over the grid\n",
    "dist_func = distance_funcs[best_dist]\n",
    "Z = np.array([knn_predict(X_train_2D, y_train, np.array([x, y]), best_k, dist_func)\n",
    "              for x, y in np.c_[xx.ravel(), yy.ravel()]])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "plt.scatter(X_train_2D[:, 0], X_train_2D[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolor='k', s=20, label=\"Train\")\n",
    "plt.scatter(X_test_2D[:, 0], X_test_2D[:, 1], c=y_test, cmap=plt.cm.coolwarm, marker='x', s=40, label=\"Test\")\n",
    "plt.title(f\"Decision Boundary (Best K={best_k}, {best_dist} Distance)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f8a9f",
   "metadata": {},
   "source": [
    "\n",
    "## Observations & Inference\n",
    "\n",
    "1. **Best Model Parameters:**  \n",
    "   - The highest test accuracy (**96.49%**) was achieved using **K = 3** and **Manhattan Distance**.  \n",
    "   - This combination gave **Precision = 1.0** and **Recall â‰ˆ 0.915**, meaning all predicted malignant cases were correct, but a few malignant cases were missed (false negatives).  \n",
    "\n",
    "2. **Effect of K on Accuracy:**  \n",
    "   - For smaller `K` values (3, 4), accuracy was generally higher.  \n",
    "   - As `K` increased (especially at `K=47`), accuracy slightly decreased for most distance metrics due to excessive smoothing and reduced sensitivity to local patterns.  \n",
    "\n",
    "3. **Effect of Distance Metrics:**  \n",
    "   - **Manhattan Distance** performed best overall, closely followed by **Euclidean** and **Minkowski**.  \n",
    "   - **Cosine Similarity** and **Hamming Distance** produced lower accuracies, suggesting that magnitude-based distances are more effective for this dataset than purely directional or binary comparison metrics.  \n",
    "\n",
    "4. **Confusion Matrix Insights:**  \n",
    "   - **True Positives (43)** and **True Negatives (67)** dominate, with **0 False Positives** (no benign tumor misclassified as malignant).  \n",
    "   - **4 False Negatives** occurred, meaning a few malignant cases were incorrectly classified as benign. This is critical in medical contexts as it might delay treatment.  \n",
    "\n",
    "5. **Decision Boundary Visualization:**  \n",
    "   - After PCA reduction to 2D, the decision boundary shows a clear separation between malignant and benign samples for the chosen model, with only minor overlaps at the class boundary.  \n",
    "\n",
    "**Final Inference:**  \n",
    "A KNN classifier with **K=3** and **Manhattan Distance** offers the best trade-off between sensitivity and specificity for this dataset. However, in medical diagnosis scenarios, minimizing false negatives should be prioritized, so future work could explore weighted KNN or adjusting decision thresholds to improve recall without sacrificing too much precision.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
